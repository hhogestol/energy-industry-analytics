---
title: "lab6_ene434"
author: "hhogestol"
date: "3/23/2022"
output:
  pdf_document:
    latex_engine: xelatex
  latex_engine: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Lab 6: Advanced time series forecasting

##Carbon permit prices and a dynamic regression model.

In the previous lab, we considered forecasting just a single time series. But of course, we are often interested in the effect of one variable on another. When we are using time series data, we can refer to this as dynamic time series regression (https://otexts.com/fpp3/dynamic.html).

The question we will try to answer in this lab is first, how prices on carbon quotas in the European emissions trading scheme (ETS) affect power prices in the Nordic market. You can read more about ETS here (https://ec.europa.eu/clima/policies/ets_en), but let us quickly review some main points about what the ETS is and the theory of why it should effect power prices (even if power is produced from near 100% carbon-free sources in the Nordic market.)

* The ETS scheme is a quantity-based “Cap and Trade” form of pricing carbon emissions.

* Under full information (you know exactly how much you want to reduce carbon and what price you need to get that to happen), it is functionally equivalent to a carbon price. (In reality, we don’t have full information, and some have argued that a carbon price would be simpler and more effective. So then why did we end up with a cap-and-trade system?)

* Each year, a certain goal is set for emissions (or equivalently emissions reductions.) Permits to emit this carbon are then distributed, and can be traded on a secondary market.

* In theory, the effectiveness of permits should be the same irrespective of whether they are given away free (which happened in the phasing-in stage of the program) or auctioned off for a price (as is increasingly the case now). Can you explain why? (You can read more about the auction process here https://ec.europa.eu/clima/policies/ets/auctioning_en)

* The price of permits is established through supply and demand on the markets for permits: Both the auction markets and secondary markets.

* Emissions permits affect power prices because they increase the marginal cost of electric generation from carbon emitting sources. We can think of carbon emissions as a costly input.

* Carbon prices will also tend to affect power prices in areas with mostly renewable and carbon-free generation because a combination of interconnectors to areas with carbon-based generation and the alternative cost that the permits create. Another way of saying this is, when carbon is priced, then carbon-free generation becomes more valuable.

Lets load some packages again, and then load in the data frame we put together in the previous lab that included carbon prices.

```{r}
library(knitr)
library(tidyverse)
library(fpp3)
library(lubridate)
```

We should have saved the file somewhere on our local disk:

```{r}
power_df = read_csv("power_df.csv")
```

```{r}
colnames(power_df)[23] = "ets_price"
```

Lets visualise our ets series again:

```{r}
ggplot(power_df, aes(x=date, y=ets_price)) +
  geom_line()
```

We see that the ETS price collapsed after 2008 and then even further after 2012. What happened in these periods that could explain the movements?

What happened around approximately 2018 that led to a resurgence of the ETS price? Has the surge in the ETS price had any immediate effects on generation in Europe? (You might want to do some searching online for information)

We can also ask what happens to carbon prices under the current crisis? You can look at the website of EMBER for an updated view

Now let us again consider prices in DK1. First, we convert our data frame to tsibble format and then we will convert the unit of prices to Euro/kWh, just so it will be of a similar scale to the ets prices (Euro/tonn carbon).

```{r}
power_df$date = yearmonth(power_df$date)
power_ts = tsibble(power_df, index=date)
power_ts = power_ts %>% mutate(
  DK1 = DK1/1000
)
```

First let us run a somewhat naive model, where the error terms of the model are modelled as an AR(2) process and we use ets as an exogenous regressor.

(Notice that I wrote that the error terms are modeled as an AR(2) term, and not the price series itself. This is the default behavior in the function ARIMA, and you can read about why here).

```{r}
armax1 = power_ts %>% fill_gaps() %>%
  model(ARIMA(DK1 ~ ets_price + pdq(2,0,0)))
report(armax1)
```

The results appear reasonable, a 1 Euro increase in the carbon price is associated with a .13 EURO increase in the electricity price (per kWh).

There are some problems with this regression model, as it stands, but let us first create some scenarios based on this estimation.
```{r}
#scenarios

#no change - take last value in series and extend by 12 months
scen1 = new_data(power_ts, 12) %>% mutate(
  ets_price = rep(power_ts$ets_price[128],12)
) 

#constant increase of 0.5 EUR per month
scen2 = new_data(power_ts, 12) %>% mutate(
  ets_price = rep(power_ts$ets_price[128],12)  +  cumsum(rep(.5,12))
) 
```

Then we create two forecasts based on our two scenarios:

```{r}
armax1_forecast1 = forecast(armax1, new_data=scen1)
armax1_forecast2 = forecast(armax1, new_data=scen2)
```

First the forecast where we have included a constant carbon permit price.

```{r}
armax1_forecast1 %>% autoplot(power_ts)
```

Does this seem reasonable?

Then the forecast with a rising CO2 price:

```{r}
armax1_forecast2 %>% autoplot(power_ts)
```

From Lab 5, we know that there is some doubt about whether we should consider the price series stationary or not, but when we include an exogenous regressor in a time series regression, the exogenous variable should also be stationary in order to give correct inference.

Looking at the carbon price series, we may well doubt whether the series is stationary. A test confirms this:

```{r}
library(tseries)
```

```{r}
adf.test(power_ts$ets_price)
```


So what we want instead is a model where we take the first difference of both the y-variable (power prices) and the x-variable (carbon permit prices).

We could do this manually - first creating the differenced series for both power prices and carbon prices, but luckily by specifying that the model is I(1) in our Arima() function, a difference of both series will automatically be done.

Here we can also run a comparison model without an exogenous regression, and check if the exogenous regressor does actually improve the goodness of fit.

```{r}
armax2 = power_ts %>% fill_gaps() %>% model(
  modWithEts = ARIMA(DK1 ~ ets_price + pdq(2,1,0)),
  modWOutEts = ARIMA(DK1 ~ pdq(2,1,0))
  )
```

We can compare the fit (AIC) of the two models:

```{r}
glance(armax2) %>% arrange(AICc)
```

What we see is that the AIC and BIC values are lower in the model with carbon prices, and this indicates a better fit to the data, so we are comfortable saying that including carbon prices improves the predictive performance of our model.

We check our residuals:

```{r}
armax2 %>% select(modWithEts) %>% gg_tsresiduals()
```

Now we again create our forecasts:

```{r}
fcast3 = armax2 %>% select(modWithEts) %>% forecast(new_data=scen1)
fcast4 = armax2 %>% select(modWithEts) %>% forecast(new_data=scen2)
```

```{r}
fcast3 %>% autoplot(power_ts)
```

```{r}
fcast4 %>% autoplot(power_ts)
```

Here we see that the higher carbon prices are associated with a prediction of a positive growth in the prices, but with quite a bit of uncertainty.


##Advanced seasonality

Now we move back to some more advanced techniques for modelling complex seasonality, which has a tendency to pop up in power markets.

We’ll load in hourly consumption data for Norway as a whole and its 5 price areas and clean it up a bit (in the previous lab we had daily consumption data):

```{r}
cons = read_csv2("http://jmaurit.github.io/analytics/labs/data/consumption-no-areas_2019_hourly.csv")
```

```{r}
#cons["Date"] = as.Date(cons$Date, format="%d/%m/%Y")
cons = cons %>% separate(Hours, sep="-", into=c("start", "end"))

#we use lubridate to create a date-time columns
cons["period"] = dmy_h(paste(cons$Date, cons$start))

#We have one missing value - I will fudge it and replace it with the previous hours value
cons[["NO"]][cons$period==as_datetime("2019-03-31 02:00:00")] = cons[["NO"]][cons$period==as_datetime("2019-03-31 01:00:00")]

#And we have one duplicate hour
duplicates(cons)
```

```{r}
dupRow = duplicates(cons)[2,]
```

```{r}
cons = cons %>% rows_delete(dupRow, by=c("period", "NO"))
```

```{r}
#convert to tsibble
cons_ts = cons %>% select("NO1":"period") %>% tsibble(index=period)
```

lets take a look:

```{r}
cons_ts %>% select(NO) %>% autoplot()
```

With so much data, it is hard to make out the patterns at smaller frequencies.

Lets try to look at a smaller interval of just a month:

```{r}
cons_ts %>%
  dplyr::filter((period>=as_datetime("2019-11-01 00:00:00")) & (period<=as_datetime("2020-01-01 00:00:00"))) %>%  autoplot(NO)
```

What we see more clearly here is two levels of seasonality. We have the daily-24 hour cycle of electricity consumption: low during the night, a spike in mid-day, and then a lull. We also have the weekly seasonality of lulls during the weekend. If we had multiple years of data, then we could also clearly see strong yearly patterns of seasonal change.

To handle the extra seasonality we will use fourier analysis, where we basically decompose a time series into sine and cosine terms of varying frequency. You can read more here.

As a simple example to start with. Lets just model the daily (24 hour) seasonality. Lets say that the short-term dynamics can be modeled as an AR2 model. Then we can write the model as:

Ct = β1Ct−1 + β2Ct−2 + sin(2πkt/24)+ cos(2πkt/24)

```{r}
#short_cons = ts(window(cons_ts, start="2019-11-01", end="2020-01-01"), frequency=24)

smod = cons_ts %>% model(
  fmod1 = ARIMA(NO ~ fourier(K=1) + pdq(2,0,0) + PDQ(0,0,0))
)


forecast1 = smod %>% forecast(h=24*14)


forecast1 %>%  autoplot(cons_ts[cons_ts$period>as_date("2019-10-01 00:00:00"),], level = 95)
```

Here we see that we get the basic daily pattern. Let us experiment with higher orders of fourier analysis in order to capture the more complicated daily seasonality. I will also leave out the pdq() parameters in order to allow the algorithm to choose the optimal ARMA dynamics, though I will specify PDQ(0,0,0) so that the seasonality is modeled through the fourier term.

```{r}
smod2 = cons_ts %>% model(
  fmod1 = ARIMA(NO ~ fourier(K=1) +  PDQ(0,0,0)),
  fmod2 = ARIMA(NO ~ fourier(K=2) +  PDQ(0,0,0)),
  fmod3 = ARIMA(NO ~ fourier(K=3) +  PDQ(0,0,0)),
  fmod4 = ARIMA(NO ~ fourier(K=4) +  PDQ(0,0,0)),
  fmod5 = ARIMA(NO ~ fourier(K=5) + PDQ(0,0,0)),
  fmod6 = ARIMA(NO ~ fourier(K=6) + PDQ(0,0,0)),
  fmod7 = ARIMA(NO ~ fourier(K=7) + PDQ(0,0,0)),
  fmod8 = ARIMA(NO ~ fourier(K=8) + PDQ(0,0,0))
)
```

```{r}
smod2 %>%
  forecast(h = 24*14) %>%
  autoplot(cons_ts[cons_ts$period>as_datetime("2019-09-01 00:00:00"),], level = 95) +
  facet_wrap(vars(.model), ncol = 2) +
  guides(colour = FALSE, fill = FALSE, level = FALSE) +
  geom_label(
    aes(x = as_datetime("2019-10-01 00:00:00"), y = 20000, label = paste0("AICc = ", format(AICc))),
    data = glance(smod2)
  )
```

The highest order of fourier analysis gave us the lowest AIC score. We could possibly investigate higher orders as well, but for now, let us look closer at an 8th order fourier model:

```{r}
#[cons_ts$period>as_datetime("2019-09-01 00:00:00"),]
smod2 %>% select(fmod8) %>% 
  forecast(h = 24*14) %>%
  autoplot(cons_ts %>% dplyr::filter(period>as_datetime("2019-09-01 00:00:00")), level = 95)
```

It doesn’t look too bad, but we notice that our forecasted time series still appears much more regular than the actual time series. This is, to a certain extent, too be expected, and that is one of the reasons for the uncertainty bands. But it may also be a sign that we need a more complex fourier model.

What we want to do now is to explicitly model both the daily and weekly seasonality. You can read more here (https://otexts.com/fpp3/complexseasonality.html)

```{r}
#create a new fourier series
smod3 = cons_ts %>%
  model(
    fmod1 = ARIMA(NO ~ PDQ(0, 0, 0) +
                fourier(period = 24, K = 8) + fourier(period = 24*7, K = 5))
  )

forecast2 = smod3 %>% forecast(h = 24*14)

forecast2 %>% autoplot(cons_ts %>% dplyr::filter(period>as_datetime("2019-09-01 00:00:00")))
```

Now the forecast looks much better. My choice of order for the two seasonal terms was somewhat arbitrary here. For multiple seasonalities, AIC will tend to overestimate the number of orders you need. In practice, you will probably have to experiment until you find orders that seem to do a good job of mimicking the seasonality.

##Vector Autoregressives models
In the first section in this lab, we included a second series as an exogenous regressor. We took it as a given that the series was in fact exogenous. That is, that a change in the price of emissions permits caused a change in power prices (in Denmark). We did not consider the possibility that higher prices caused higher permit prices. Or, more realistically, that both higher permit prices and power prices were caused by an unobserved variable. For example, that a stronger European economy causes both higher emissions prices and higher power prices. If this were the case, we could no longer give a causal interpretation to our regression. Worse, our forecast scenarios of power prices given higher emissions prices might be misleading.

Such endogeneity problems can be fiendishly hard to sort out. One tool we can use to try to get some clarity is called a Vector Autoregressive Model (VAR). This allows us to treat essentially every one of the variables as an endogenous variable: affecting and being affected by the other variables.

VAR models can give us forecasts that are more realistic and display more complex dynamics. The downside is that these models can be difficult to interpret.


##Daily consumption and price

We’ll take an almost obvious example: prices and consumption on the power market. We know that that periods of high consumption can lead to high prices. But we also know that high prices will probably affect consumption, even though electricity markets are known to be quite price inelastic in the short term (why?).

So lets import daily consumption and price data.

```{r}
cons_daily = read_csv2("http://jmaurit.github.io/analytics/labs/data/consumption-per-country_2019_daily.csv")
```

```{r}
cons_daily["date"] = as.Date(cons_daily$date, format="%d/%m/%Y")
```

```{r}
prices_daily = read_csv2("http://jmaurit.github.io/analytics/labs/data/elspot_prices_2019_daily.csv")
```

```{r}
prices_daily["date"] = as.Date(prices_daily$date, format="%d/%m/%Y")
```

Then, selecting out consumption for Norway, and prices in Oslo, we join the series into a data frame we call NO_df.

```{r}
NO_df = prices_daily %>% dplyr::select(date, Oslo) %>% inner_join(dplyr::select(cons_daily, date, NO), by="date")
colnames(NO_df)[2:3] = c("Oslo_price_EUR_MWH", "NO_cons_MWH")
```

Now, instead of a model of consumption on price or price on consumption, we want to allow each to affect each other. So a model with one lag would look like:

Ct = a1 + ϕ11Ct−1 + ϕ12Pt−1 + ϵ1,t Pt = a2 + ϕ21Ct−1 + ϕ22Pt−1 + ϵ1,t

We need to put our series into tibble format:

```{r}
NO_ts = tsibble(NO_df, index=date)
```


By now we can probably guess that our price and consumption series will not be stationary, so in our modeling we will have to difference the data. We also transform to a log to both make the series a bit more linear and to give our regression coefficients an interpretation of elasticities.

```{r}
NO_ts = NO_ts %>% mutate(
  log_dprice = difference(log(Oslo_price_EUR_MWH)), 
  log_dcons = difference(log(NO_cons_MWH))
)
```

```{r}
NO_ts %>% select(log_dprice) %>% autoplot()
```

```{r}
NO_ts %>% select(log_dcons) %>% autoplot()
```

Here we fit our model, allowing the VAR algorithm automatically choose the optimal number of lags for each variable. I use the report() command to show the estimates, but in general, it is hard to give any meaningful economic interpretations to such VAR models.

```{r}
varMod = NO_ts %>%
  model(
    mod1 = VAR(vars(log_dprice, log_dcons))
  )

varMod %>% report()
```

We can look at the autocorrelations of our residuals from our model:

```{r}
varMod %>%
  augment() %>%
  ACF(.innov) %>%
  autoplot()
```

We can see that we failed to fully account for the weekly seasonality in our consumption data. But we will leave that aside for now and look at our forecasts:

```{r}
varMod %>%
  forecast(h=14) %>%
  autoplot(NO_ts %>% dplyr::filter(date>as_date("2019-09-01")))
```

We of course get our forecast in differences as well, but it would be easy enough to transform this into levels again.


##Assignment

*1. In a dynamic regression model, it may make sense to include lagged variables as exogenous regressors. In the model of DK1 prices, include both contemporaneous and lagged carbon permit prices. How does this change your model? (You may want to read Ch 10.6 in fpp3).*


```{r}
### DK1 with contemporaneous and lagged carbon permit prices ###

# Comparison of carbon price lags

armax3 = power_ts %>% fill_gaps() %>% model(
  modWithEts = ARIMA(DK1 ~ ets_price),
  modWithLag1 = ARIMA(DK1 ~ ets_price + 
                        lag(ets_price,1)),
  modWithLag2 = ARIMA(DK1 ~ ets_price + 
                        lag(ets_price,1) + 
                        lag(ets_price,2)),
  modWithLag3 = ARIMA(DK1 ~ ets_price + 
                        lag(ets_price,1) + 
                        lag(ets_price,2) + 
                        lag(ets_price,3)),
  modWithLag4 = ARIMA(DK1 ~ ets_price + 
                        lag(ets_price,1) + 
                        lag(ets_price,2) + 
                        lag(ets_price,3) + 
                        lag(ets_price,4)),
  modWithLag5 = ARIMA(DK1 ~ ets_price + 
                        lag(ets_price,1) + 
                        lag(ets_price,2) + 
                        lag(ets_price,3) + 
                        lag(ets_price,4) + 
                        lag(ets_price,5)),
  modWithLag6 = ARIMA(DK1 ~ ets_price + 
                         lag(ets_price,1) + 
                         lag(ets_price,2) + 
                         lag(ets_price,3) + 
                         lag(ets_price,4) + 
                         lag(ets_price,5) + 
                         lag(ets_price,6)),
  modWithLag7 = ARIMA(DK1 ~ ets_price + 
                        lag(ets_price,1) + 
                        lag(ets_price,2) + 
                        lag(ets_price,3) + 
                        lag(ets_price,4) + 
                        lag(ets_price,5) + 
                        lag(ets_price,6) + 
                        lag(ets_price,7)),
  modWithLag8 = ARIMA(DK1 ~ ets_price + 
                        lag(ets_price,1) + 
                        lag(ets_price,2) + 
                        lag(ets_price,3) + 
                        lag(ets_price,4) + 
                        lag(ets_price,5) + 
                        lag(ets_price,6) + 
                        lag(ets_price,7) + 
                        lag(ets_price,8))
  )

# Looking at the data

glance(armax3)                                   # Modified with Lag 8 has the lowest AICc

# Selecting the model with the lowest AICc/BIC 

armax3 %>% select(modWithLag8) %>% report()     # The selected models is LM with ARIMA(1,1,0) errors
                                                # AIC = 148.95, AICc = 151.23, BIC = 180.33

# Creating forecasts with the identified ARIMA model

fcast5 = armax3 %>% 
  select(modWithLag8) %>%
  forecast(new_data = scen1)
  
fcast6 = armax3 %>%
  select(modWithLag8) %>%
  forecast(new_data = scen2)

```

```{r}
# Plot of forecast 5

fcast5 %>% autoplot(power_ts) +
  ggtitle("Forecast of scenario 1 with lagged carbon prices") +
  xlab("Date") +
  ylab("DK1 Price") +
  scale_color_manual(values = c("blue"),
                     breaks = c("Scen. 1 w/ lag 8"))
```

With scenario 1, the forecast is an increase in price and then leveling out at a steady price level just north of 5.

```{r}
# Plot of forecast 6

fcast6 %>% autoplot(power_ts) +
  ggtitle("Forecast of scenario 2 with lagged carbon prices") +
  xlab("Date") +
  ylab("DK1 Price") +
  scale_color_manual(values = c("blue"),
                     breaks = c("Scen. 2 w/ lag 8"))
```
The next forecast sharply increases from just below 5 to around 5.5 in a short time span, before it swings slightly and then steadily increases towards ~5.75. 

More generally:
The forecasts look more refined than forecasts 1-4. They are less rigid in their path, which indicates that they could be more accurate compared to those previous forecasts.


*2. From ENTSOE-E, download hourly consumption data for Norway for 2017 and 2018. Join this with the 2019 data in order to create one long time series for Norwegian consumption. Then model the seasonality in the data (at monthly, weakly and daily level), with fourier terms.*

```{r}
### Seasonality of the Norwegian Hourly Consumption Data with Fourier Terms ###

# Load hourly consumption data for 2017-2018
library(readxl)
cons_Nor = read_excel("ProductionConsumption-2017-2018.xlsx")

# Remove production variable
cons_Nor = subset(cons_Nor, select = -c(2))

# Rename columns
colnames(cons_Nor) <- c("time", "value")

# Set 2017-2018 time as datetimeUTC
cons_Nor$time <- as_datetime(cons_Nor$time)

# Remove redundant columns in 2019 data to fit the 2017-2018 data
cons_Nor19 = subset(cons, select = -c(1:8))

# Rename columns in 2019
colnames(cons_Nor19) <- c("value","time")

# Change order of 2019 colums
cons_Nor19 <- cons_Nor19[c("time", "value")]


# Join with the 2019 data (cons)
cons_comb <- bind_rows(cons_Nor, cons_Nor19)

# Checking for duplicates
duplicates(cons_comb)

# Removing duplicates
dupRows = duplicates(cons_comb)

cons_comb = cons_comb %>% rows_delete(dupRows, by=c("time", "value"))

# Create a tsibble of the combined data
consComb_ts <- cons_comb %>% as_tsibble()                                         

```

```{r}
## Modelling the seasonality with fourier terms ##

# Model with monthly, weekly and daily levels

# Monthly level
fourier_monthly = consComb_ts %>%
  model(TSLM(value ~ trend() + fourier(period = 24*7*4, K = 3)))
report(fourier_monthly)

# Weekly level
fourier_weekly = consComb_ts %>%
  model(TSLM(value ~ trend() + fourier(period = 24*7, K = 5)))
report(fourier_weekly)

# Daily level
fourier_daily = consComb_ts %>%
  model(TSLM(value ~ trend() + fourier(period = 24, K = 8)))
report(fourier_daily)

# Combining the monthly, weekly, and daily levels into one model

smod4 = consComb_ts %>% 
  fill_gaps() %>%
  model(consComb_mod = ARIMA(consComb_ts ~ PDQ(0, 0, 0) +
                  fourier_monthly + 
                  fourier_weekly + 
                  fourier_daily))
```

```{r}
## Forecast ##

# Forecasting the model one year ahead
forecast3 = smod4 %>% forecast(h = 24*365)

# Plotting the forecast from January 1, 2020

autoplot(forecast3) +
  ggtitle("Forecast one year ahead") +
  xlab("Date") +
  ylab("Consumption") +
  scale_color_manual(values = c("blue", "red"),
                      breaks = c("Forecast", "Historic consumption")
  )
```


*3. Create a VAR model for consumption and prices in 2019 using Danish data (You can find it on ENTSOE_E or at the Danish TSO’s energy data site. Create a 30 day forecast. Load in actual data for january 2020–how does your forecast look? Include wind power in Denmark as a variable. How does this affect the model and forecast?*


```{r}
### VAR model ###

# Load data
wtDF = read_csv("https://jmaurit.github.io/analytics/labs/data/wtDF.csv")

#Formatting the data and creating a tsibble
dk_DF = wtDF %>% filter(area %in% c("DK1"))
dk_DF = dk_DF %>% select(-area)
dk_DF = dk_DF %>% pivot_wider(id_cols=time ,names_from=variable)
dk_DF = dk_DF %>% filter(!is.na(time))
dk_ts = as_tsibble(dk_DF)

dk_ts = dk_ts %>% filter(time>as_datetime("2019-01-01 00:00:00"))
dk_ts = dk_ts %>% filter(time<as_datetime("2020-01-01 00:00:00"))


# VAR models
dk_ts = dk_ts %>% filter(!is.na(Consumption))
dk_ts = dk_ts %>% fill_gaps()


varMod2 = dk_ts %>%
  model(
    mod1 = VAR(vars(Consumption,  Prices))
  )

varMod2 %>% report()

```


```{r}
varMod3 = dk_ts%>%
  model(
    mod1 = VAR(vars(Consumption, Prices, Wind))
  )

varMod3 %>% report()
```


```{r}
# Forecast consumption and prices

varMod2 %>%
  forecast(h=7*24*4) %>%
  autoplot(dk_ts %>% filter(time<as_datetime("2020-01-31 00:00:00")))


```

As the forecast suggests, the consumption will stay within seasonal range and the prices may increase somewhat.


```{r}
# Forecast consumption, prices and wind

varMod3 %>%
  forecast(h=7*24*4) %>%
  autoplot(dk_ts %>% filter(time<as_datetime("2020-01-31 00:00:00")))
```

By adding the wind variable, the forecast takes a greater degree of variance into account. The uncertainty increases, but the as illustrated, the forecasted level remains steady.
Furthermore, the increased variance seems unfounded, seeing how wind power should not affect power consumption and prices to this extent. As noted in the curriculum, wind power is mostly "dependent on the wind speed at any given time(...)" (Mauritzen, "Lab 6: Advanced time series forecasting", 2022).






